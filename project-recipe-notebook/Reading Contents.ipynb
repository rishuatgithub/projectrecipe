{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Recipe Notebook\n",
    "### Scrapping Web pages for getting the recipe details\n",
    "- Author: Rishu Shrivastava (@rishuatgithub)\n",
    "- Last Updated: Sunday May 3, 2020\n",
    "#### License: Copyrighted. Rishu Kumar Shrivastava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "web_urls=['https://www.jamieoliver.com/recipes/']\n",
    "\n",
    "#response = [requests.get(r) for r in web_urls]\n",
    "\n",
    "class WebConnection:\n",
    "    \n",
    "    def __init__(self,base_url,url):\n",
    "        self.base_url = base_url\n",
    "        if base_url in url: ## if the url contains the base url\n",
    "            self.url = url\n",
    "        else:\n",
    "            self.url = base_url + url\n",
    "            \n",
    "        print(f\"Parsing web url: {self.url}\")\n",
    "    \n",
    "    def getresponse(self):\n",
    "        response = requests.get(self.url)\n",
    "        return response\n",
    "    \n",
    "    def getcontent(self):\n",
    "        response = self.getresponse()\n",
    "        content = BeautifulSoup(response.content)\n",
    "        return content\n",
    "    \n",
    "\n",
    "#connectWebObj = WebConnection(web_urls[0])\n",
    "#con = connectWebObj.getcontent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse the website and iterate through every layer to generate the mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_strings = {'l0':'tile-wrapper','l1':'recipe-block','l2':'recipe-block'}\n",
    "search_title_strings = {'l0':'tile-title','l1':'recipe-title','l2':'recipe-title'}\n",
    "\n",
    "def parsingJOWeb():\n",
    "    \n",
    "    url = web_urls[0]\n",
    "    base_url = urljoin(web_urls[0],'/') ## https://jamieoliver.com/\n",
    "    \n",
    "    web_parsed_content = {}\n",
    "    web_parsed_content['levels'] = {}\n",
    "    \n",
    "    l1_arr = []\n",
    "    l2_arr = []\n",
    "    l3_arr = []\n",
    "    \n",
    "    cont = WebConnection(base_url,url).getcontent()\n",
    "    web_parsed_content['title'] = cont.title.text\n",
    "    web_parsed_content['parent_url'] = url\n",
    "    \n",
    "    ## level 0\n",
    "    l0_block = cont.find_all(\"div\",{'class':search_strings['l0']})\n",
    "    l0_ahref = [f.find_all(\"a\")[0].get(\"href\") for f in l0_block]\n",
    "    l0_title = [f.find(\"div\",{'class':search_title_strings['l0']}).text for f in l0_block]\n",
    "    \n",
    "    web_parsed_content['levels']['level0'] = {'parent_url':url, 'level':{'title':l0_title,'url':l0_ahref}}\n",
    "    \n",
    "    ## level 1\n",
    "    for l0 in l0_ahref:\n",
    "        cont_l1 = WebConnection(base_url,l0).getcontent()\n",
    "        l1_block = cont_l1.find_all(\"div\",{'class':search_strings['l1']})\n",
    "        l1_ahref = [f.find_all(\"a\")[0].get(\"href\") for f in l1_block]\n",
    "        l1_title = [f.find(\"div\",{'class':search_title_strings['l1']}).text for f in l1_block]\n",
    "        l1_arr.append({'parent_url':l0, 'level':{'title':l1_title,'url':l1_ahref}})\n",
    "        \n",
    "    web_parsed_content['levels']['level1'] = l1_arr\n",
    "    \n",
    "    \n",
    "    ## level 2\n",
    "    l1_url_list = [ pcl1['level']['url'] for pcl1 in web_parsed_content['levels']['level1']]\n",
    "    \n",
    "    for l1_list in l1_url_list:\n",
    "        for l1 in l1_list:\n",
    "            cont_l2 = WebConnection(base_url,l1).getcontent()\n",
    "            l2_block = cont_l2.find_all(\"div\",{'class':search_strings['l2']})\n",
    "            l2_ahref = [f.find_all(\"a\")[0].get(\"href\") for f in l2_block]\n",
    "            l2_title = [f.find(\"div\",{'class':search_title_strings['l2']}).text for f in l2_block]\n",
    "            #l2_meta = [f.find(\"div\",{'class':'recipe-meta'}).text for f in l1_block]\n",
    "            l2_arr.append({'parent_url':l1, 'level':{'title':l2_title,'url':l2_ahref}})\n",
    "    \n",
    "    web_parsed_content['levels']['level2'] = l2_arr\n",
    "    \n",
    "    \n",
    "    ## level 3 - final level\n",
    "    l2_url_list = [ pcl2['level']['url'] for pcl2 in web_parsed_content['levels']['level2']]\n",
    "    \n",
    "    for l2_list in l2_url_list:\n",
    "        for l2 in l2_list:\n",
    "            conn = WebConnection(base_url,l2).getcontent()\n",
    "            \n",
    "            if conn.find('div',{'class':'single-recipe-details'}) != None:\n",
    "                recipe_name = conn.find('div',{'class':'single-recipe-details'}).find('h1').text\n",
    "            else:\n",
    "                recipe_name = ''\n",
    "                \n",
    "            if conn.find('div',{'class':'single-recipe-details'}) != None and conn.find('div',{'class':'single-recipe-details'}).find('p') != None:\n",
    "                recipe_subtitle = conn.find('div',{'class':'single-recipe-details'}).find('p').text\n",
    "            else:\n",
    "                recipe_subtitle = ''\n",
    "            \n",
    "            if conn.find('div',{'class':'recipe-intro'}) != None:\n",
    "                recipe_intro = conn.find('div',{'class':'recipe-intro'}).text.strip()\n",
    "            else:\n",
    "                recipe_intro = ''\n",
    "            \n",
    "            if conn.find('ul',{'class':'special-diets-list'}) != None and conn.find('ul',{'class':'special-diets-list'}).find_all('span',{'class':'full-name'}) != None:\n",
    "                title_tags = [ title_tags.text.strip() for title_tags in conn.find('ul',{'class':'special-diets-list'}).find_all('span',{'class':'full-name'})]\n",
    "            else:\n",
    "                title_tags = []\n",
    "            \n",
    "            if conn.find('div',{'class':'nutrition-expanded'}) != None and  conn.find('div',{'class':'nutrition-expanded'}).find_all('div',{'class':'inner'}) != None:\n",
    "                nutrition = [ (nut.find('span',{'class':'title'}).text.strip(), nut.find('span',{'class':'top'}).text.strip(), nut.find('span',{'class':'bottom'}).text.strip()) for nut in conn.find('div',{'class':'nutrition-expanded'}).find_all('div',{'class':'inner'})]\n",
    "            else:\n",
    "                nutrition = []\n",
    "            \n",
    "            if conn.find('div',{'class':'recipe-detail serves'}) != None:\n",
    "                servings = conn.find('div',{'class':'recipe-detail serves'}).text.strip()\n",
    "            else:\n",
    "                servings = ''\n",
    "               \n",
    "            if conn.find('div',{'class':'recipe-detail time'}) != None:\n",
    "                timing = conn.find('div',{'class':'recipe-detail time'}).text.strip().replace('Cooks In','')\n",
    "            else:\n",
    "                timing = ''\n",
    "            \n",
    "            if conn.find('div',{'class':['difficulty']}) != None:\n",
    "                difficulty = conn.find('div',{'class':['difficulty']}).text.strip().replace('Difficulty','')\n",
    "            else:\n",
    "                difficulty = ''\n",
    "            \n",
    "            if conn.find('div',{'class':'tags-list'}) != None and conn.find('div',{'class':'tags-list'}).find_all('a') != None:\n",
    "                tags_list = [ taglist.text for taglist in conn.find('div',{'class':'tags-list'}).find_all('a')]\n",
    "            else:\n",
    "                tags_list = []\n",
    "            \n",
    "            if conn.find(\"ul\",{'class':'ingred-list'}) != None and conn.find(\"ul\",{'class':'ingred-list'}).find_all('li') != None:\n",
    "                ingrednt = [''.join(t.text.strip().split(' '*11)) for t in conn.find(\"ul\",{'class':'ingred-list'}).find_all('li')]\n",
    "            else:\n",
    "                ingrednt = []\n",
    "            \n",
    "            #method = [ (index, meth.text.strip()) for index,meth in enumerate(conn.find('ol',{'class':'recipeSteps'}).find_all('li'))]\n",
    "            if conn.find('div',{'class':['instructions-wrapper']}) != None and conn.find('div',{'class':['instructions-wrapper']}).find_next().find_next().find_next() != None:\n",
    "                method = conn.find('div',{'class':['instructions-wrapper']}).find_next().find_next().find_next().text.strip().split('\\r\\n')\n",
    "            else:\n",
    "                method = []\n",
    "                \n",
    "            recipedict = {'recipe_name':recipe_name,'recipie_subtitle':recipe_subtitle,'recipe_intro':recipe_intro,\n",
    "                          'title_tags':title_tags,'tags_list':tags_list, 'servings':servings, 'timing':timing,\n",
    "                          'difficulty':difficulty, 'nutrition':nutrition, 'ingredients':ingrednt, 'method':method\n",
    "                         }\n",
    "            \n",
    "            l3_arr.append({'parent_url':l2, 'recipe_details':recipedict})\n",
    "    \n",
    "    return web_parsed_content\n",
    "    \n",
    "    \n",
    "#parsingJOWeb()\n",
    "\n",
    "data_dict = {'data': parsingJOWeb() }  # wrap everything into a dict\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to a file locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save to a file \n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"jamieoliverdata.json\",\"w\") as file:\n",
    "    file.write(json.dumps(data_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test stuff - recipie extracts\n",
    "\n",
    "#url = 'https://www.jamieoliver.com//recipes/vegetables-recipes/classic-ratatouille/'\n",
    "url = 'https://www.jamieoliver.com//recipes/vegetable-recipes/best-ever-brussels-sprouts/'\n",
    "#url = 'https://www.jamieoliver.com//recipes/vegetables-recipes/whole-roasted-miso-aubergine/'\n",
    "conn = WebConnection('https://www.jamieoliver.com/',url).getcontent()\n",
    "\n",
    "\n",
    "recipe_name = conn.find('div',{'class':'single-recipe-details'}).find('h1').text\n",
    "recipe_subtitle = conn.find('div',{'class':'single-recipe-details'}).find('p').text\n",
    "recipe_intro = conn.find('div',{'class':'recipe-intro'}).text.strip()\n",
    "title_tags = [ title_tags.text.strip() for title_tags in conn.find('ul',{'class':'special-diets-list'}).find_all('span',{'class':'full-name'})]\n",
    "nutrition = [ (nut.find('span',{'class':'title'}).text.strip(), nut.find('span',{'class':'top'}).text.strip(), nut.find('span',{'class':'bottom'}).text.strip()) for nut in conn.find('div',{'class':'nutrition-expanded'}).find_all('div',{'class':'inner'})]\n",
    "servings = conn.find('div',{'class':'recipe-detail serves'}).text.strip()\n",
    "timing = conn.find('div',{'class':'recipe-detail time'}).text.strip().replace('Cooks In','')\n",
    "difficulty = conn.find('div',{'class':['difficulty']}).text.strip().replace('Difficulty','')\n",
    "tags_list = [ taglist.text for taglist in conn.find('div',{'class':'tags-list'}).find_all('a')]\n",
    "ingrednt = [''.join(t.text.strip().split(' '*11)) for t in conn.find(\"ul\",{'class':'ingred-list'}).find_all('li')]\n",
    "\n",
    "method = conn.find('div',{'class':['instructions-wrapper']}).find_next().find_next().find_next().text.strip().split('\\r\\n')\n",
    "\n",
    "\n",
    "#final = {'recipe_name':recipe_name,'recipie_subtitle':recipe_subtitle,'recipe_intro':recipe_intro,\n",
    "#         'tags_list':tags_list, 'servings':servings, 'timing':timing,\n",
    "#         'difficulty':difficulty, 'nutrition':nutrition, 'ingredients':ingrednt, 'method':method\n",
    "#        }\n",
    "\n",
    "#final\n",
    "\n",
    "\n",
    "\n",
    "#recipe_name\n",
    "\n",
    "conn.find('div',{'class':'single-recipe-details'}) != None and conn.find('div',{'class':'single-recipe-details'}).find('p') != None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
